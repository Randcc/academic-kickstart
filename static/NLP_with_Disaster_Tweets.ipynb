{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1215b2c88>,\n",
       "  <matplotlib.axis.XTick at 0x1215b2438>],\n",
       " <a list of 2 Text xticklabel objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='target', data= train)\n",
    "Labels= ('No Disaster', 'Real Disaster')\n",
    "plt.xticks(range(2), Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.01875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.11250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.60000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity\n",
       "0   0.00000\n",
       "1   0.10000\n",
       "2  -0.01875\n",
       "3   0.00000\n",
       "4   0.00000\n",
       "5  -0.11250\n",
       "6  -0.20000\n",
       "7   0.50000\n",
       "8   0.00000\n",
       "9  -0.60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/analyze-tweet-sentiments-in-python/\n",
    "from textblob import TextBlob\n",
    "# Create textblob objects of the tweets\n",
    "sentiment_objects = [TextBlob(tw) for tw in train['text']]\n",
    "# Create list of polarity valuesx and tweet text\n",
    "sentiment_values = [[tweet.sentiment.polarity] for tweet in sentiment_objects]\n",
    "\n",
    "#polarity values that range from 1 to -1.\n",
    "#Values closer to 1 indicate more positivity, while values closer to -1 indicate more negativity.\n",
    "sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\"])\n",
    "sentiment_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity(_df):\n",
    "    _df = pd.concat([_df, sentiment_df], axis=1)\n",
    "    return _df\n",
    "train=add_polarity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.01875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  polarity  \n",
       "0       1   0.00000  \n",
       "1       1   0.10000  \n",
       "2       1  -0.01875  \n",
       "3       1   0.00000  \n",
       "4       1   0.00000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#del tweet_disaster['polarity']\n",
    "#Analyze Sentiments Using Twitter Data\n",
    "# Remove polarity values equal to zero\n",
    "import matplotlib.pyplot as plt\n",
    "sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Plot histogram with break at zero\n",
    "sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "\n",
    "plt.title(\"Real or Not? Disaster Tweets/Polarity \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rand/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rand/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP tweets: Cleaning & Preprocessing tweets Data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_1=[]\n",
    "for tw in train[\"text\"]:\n",
    "    word_tokens = word_tokenize(tw) \n",
    "    #Delete ponctuation\n",
    "    word_tokens=[word.lower() for word in word_tokens if word.isalpha()]\n",
    "    #Delete stop words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words : \n",
    "            if  w!='http':\n",
    "                filtered_sentence.append(w) \n",
    "               \n",
    "    #print(word_tokens) \n",
    "    #print(filtered_sentence) \n",
    "    \n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    #print(filtered_sentence)\n",
    "    #print(Stem_words)\n",
    "    #https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "    lem=[]\n",
    "    for w in filtered_sentence:\n",
    "        word1 = Word(w).lemmatize(\"n\")\n",
    "        word2 = Word(word1).lemmatize(\"v\")\n",
    "        word3 = Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    tweet_1.append(lem)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity_lem</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>['resident', 'ask', 'place', 'notify', 'office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.125</td>\n",
       "      <td>['rockyfire', 'update', 'california', 'hwy', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.200</td>\n",
       "      <td>['flood', 'disaster', 'heavy', 'rain', 'cause'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.500</td>\n",
       "      <td>['top', 'hill', 'see', 'fire', 'wood']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>['emergency', 'evacuation', 'happen', 'build',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>['afraid', 'tornado', 'come', 'area']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity_lem                                               lems\n",
       "0         0.000  ['deed', 'reason', 'earthquake', 'may', 'allah...\n",
       "1         0.100  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...\n",
       "2         0.000  ['resident', 'ask', 'place', 'notify', 'office...\n",
       "3         0.000  ['people', 'receive', 'wildfire', 'evacuation'...\n",
       "4         0.000  ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...\n",
       "5        -0.125  ['rockyfire', 'update', 'california', 'hwy', '...\n",
       "6        -0.200  ['flood', 'disaster', 'heavy', 'rain', 'cause'...\n",
       "7         0.500             ['top', 'hill', 'see', 'fire', 'wood']\n",
       "8         0.000  ['emergency', 'evacuation', 'happen', 'build',...\n",
       "9        -0.600              ['afraid', 'tornado', 'come', 'area']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_objects = [TextBlob(str(t)) for t in tweet_1]\n",
    "sentiment_values = [[tweet_1.sentiment.polarity, str(tweet_1)] for tweet_1 in sentiment_objects]\n",
    "sentiment_values[0]\n",
    "sentiment_df1 = pd.DataFrame(sentiment_values, columns=[\"polarity_lem\", \"lems\"])\n",
    "sentiment_df1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity1(_df):\n",
    "    _df = pd.concat([_df, sentiment_df1[\"lems\"]], axis=1)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=add_polarity1(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polarity</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.01875</td>\n",
       "      <td>['resident', 'ask', 'place', 'notify', 'office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  polarity                                               lems  \n",
       "0       1   0.00000  ['deed', 'reason', 'earthquake', 'may', 'allah...  \n",
       "1       1   0.10000  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...  \n",
       "2       1  -0.01875  ['resident', 'ask', 'place', 'notify', 'office...  \n",
       "3       1   0.00000  ['people', 'receive', 'wildfire', 'evacuation'...  \n",
       "4       1   0.00000  ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polarity</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>deed  reason  earthquake  may  allah  forgive  u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>forest  fire  near  la  ronge  sask  canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.01875</td>\n",
       "      <td>resident  ask  place  notify  officer  evacuat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>people  receive  wildfire  evacuation  order  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>get  send  photo  ruby  alaska  smoke  wildfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  polarity                                               lems  \n",
       "0       1   0.00000   deed  reason  earthquake  may  allah  forgive  u  \n",
       "1       1   0.10000        forest  fire  near  la  ronge  sask  canada  \n",
       "2       1  -0.01875  resident  ask  place  notify  officer  evacuat...  \n",
       "3       1   0.00000  people  receive  wildfire  evacuation  order  ...  \n",
       "4       1   0.00000  get  send  photo  ruby  alaska  smoke  wildfir...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"lems\"]= train[\"lems\"].str.replace(\"[\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\"]\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\"\\'\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\",\", \" \") \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP tweets: Cleaning & Preprocessing tweets Data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_2=[]\n",
    "for tw in test[\"text\"]:\n",
    "    word_tokens = word_tokenize(tw) \n",
    "    #Delete ponctuation\n",
    "    word_tokens=[word.lower() for word in word_tokens if word.isalpha()]\n",
    "    #Delete stop words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words : \n",
    "            if  w!='http':\n",
    "                filtered_sentence.append(w) \n",
    "               \n",
    "    #print(word_tokens) \n",
    "    #print(filtered_sentence) \n",
    "    \n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    #print(filtered_sentence)\n",
    "    #print(Stem_words)\n",
    "    #https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "    lem=[]\n",
    "    for w in filtered_sentence:\n",
    "        word1 = Word(w).lemmatize(\"n\")\n",
    "        word2 = Word(word1).lemmatize(\"v\")\n",
    "        word3 = Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    tweet_2.append(lem)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity_lem</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>['happen', 'terrible', 'car', 'crash']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>['hear', 'earthquake', 'different', 'city', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>['forest', 'fire', 'spot', 'pond', 'goose', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.40</td>\n",
       "      <td>['apocalypse', 'light', 'spokane', 'wildfire']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>['typhoon', 'soudelor', 'kill', 'china', 'taiw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>['shake', 'earthquake']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>['probably', 'still', 'show', 'life', 'arsenal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>['hey']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.60</td>\n",
       "      <td>['nice', 'hat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.40</td>\n",
       "      <td>['fuck']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity_lem                                               lems\n",
       "0         -1.00             ['happen', 'terrible', 'car', 'crash']\n",
       "1          0.25  ['hear', 'earthquake', 'different', 'city', 's...\n",
       "2          0.00  ['forest', 'fire', 'spot', 'pond', 'goose', 'f...\n",
       "3          0.40     ['apocalypse', 'light', 'spokane', 'wildfire']\n",
       "4          0.00  ['typhoon', 'soudelor', 'kill', 'china', 'taiw...\n",
       "5          0.00                            ['shake', 'earthquake']\n",
       "6          0.00  ['probably', 'still', 'show', 'life', 'arsenal...\n",
       "7          0.00                                            ['hey']\n",
       "8          0.60                                    ['nice', 'hat']\n",
       "9         -0.40                                           ['fuck']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_objects = [TextBlob(str(t)) for t in tweet_2]\n",
    "sentiment_values = [[tweet_2.sentiment.polarity, str(tweet_2)] for tweet_2 in sentiment_objects]\n",
    "sentiment_values[0]\n",
    "sentiment_df1 = pd.DataFrame(sentiment_values, columns=[\"polarity_lem\", \"lems\"])\n",
    "sentiment_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity2(_df):\n",
    "    _df = pd.concat([_df, sentiment_df1], axis=1)\n",
    "    return _df\n",
    "test=add_polarity2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity_lem</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>happen  terrible  car  crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>hear  earthquake  different  city  stay  safe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>forest  fire  spot  pond  goose  flee  across ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>0.40</td>\n",
       "      <td>apocalypse  light  spokane  wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>typhoon  soudelor  kill  china  taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   polarity_lem                                               lems  \n",
       "0         -1.00                       happen  terrible  car  crash  \n",
       "1          0.25  hear  earthquake  different  city  stay  safe ...  \n",
       "2          0.00  forest  fire  spot  pond  goose  flee  across ...  \n",
       "3          0.40               apocalypse  light  spokane  wildfire  \n",
       "4          0.00             typhoon  soudelor  kill  china  taiwan  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"lems\"]= test[\"lems\"].str.replace(\"[\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\"]\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\"\\'\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\",\", \" \") \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use skcikit-learn's CountVectorizer to count the words in each tweet and turn them into data our\n",
    "#machine learning model can process.\n",
    "#CountVectorizer - Convert a collection of text documents to a matrix of token counts\n",
    "#and an integer count for the number of times each word appeared in the document.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "#count_vectorizer.fit(train[\"lems\"])\n",
    "# Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "#X_train_count_vec = count_vectorizer.transform(train[\"lems\"])\n",
    "#X_test_count_vec = count_vectorizer.transform(test[\"lems\"])\n",
    "X_train = count_vectorizer.fit_transform(train[\"lems\"])\n",
    "X_test = count_vectorizer.transform(test[\"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 12393)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test .shape)\n",
    "print(type(X_test ))\n",
    "print(X_test.toarray())\n",
    "#essai----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer - Convert text to word frequency vectors.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train[\"lems\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test[\"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59885833, 0.55169082, 0.62944162])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, X_train, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "        tol=0.001)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.61447721, 0.58485898, 0.63438735])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression on Count Vectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logistic_reg= LogisticRegression()\n",
    "scores = model_selection.cross_val_score(logistic_reg, X_train, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(X_train,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic Regression on tfidf\n",
    "logistic_reg_tfidf = LogisticRegression()\n",
    "scores = model_selection.cross_val_score(logistic_reg_tfidf, X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "logistic_reg_tfidf.fit(X_train_tfidf,train[\"target\"])\n",
    "y_predicted_logistic_reg_tfidf = logistic_reg_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68124709, 0.63862661, 0.71506504])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(multinomial_naive_bayes, X_train, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_naive_bayes.fit( X_train, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes on tfidf\n",
    "multinomial_naive_bayes_tfidf = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(multinomial_naive_bayes_tfidf, X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "multinomial_naive_bayes_tfidf.fit(X_train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM on Count Vector\n",
    "# Since SVM takes a lot of time, we will reduce the number of features using SVD and standardize the data before applying SVM.\n",
    "# Apply SVD, 120-200 components are good enough for SVM model.\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "xtrain_svd = svd.fit_transform(X_train)\n",
    "xtest_svd= svd.transform(X_test)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = StandardScaler()\n",
    "xtrain_svd_scl = scl.fit_transform(xtrain_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)\n",
    "# Fitting a simple SVM\n",
    "svm = SVC()\n",
    "scores = model_selection.cross_val_score(svm , xtrain_svd_scl, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "svm.fit(xtrain_svd_scl,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM on tfidf\n",
    "svd_tfidf = TruncatedSVD(n_components=120)\n",
    "xtrain_svd_tfidf = svd_tfidf.fit_transform(X_train_tfidf)\n",
    "xtest_svd_tfidf = svd_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl_tfidf = StandardScaler()\n",
    "xtrain_svd_scl_tfidf = scl_tfidf.fit_transform(xtrain_svd_tfidf)\n",
    "xtest_svd_scl_tfidf = scl_tfidf.transform(xtest_svd_tfidf)\n",
    "\n",
    "# Fitting a simple SVM\n",
    "svm_tfidf = SVC()\n",
    "scores = model_selection.cross_val_score(svm_tfidf , xtrain_svd_scl_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "svm_tfidf.fit(xtrain_svd_scl_tfidf,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on Count Vector\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier, X_train, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "xgb_classifier.fit(X_train,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on tfidf\n",
    "xgb_classifier_tfidf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_tfidf , X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "xgb_classifier_tfidf.fit(X_train_tfidf,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on Count Vector svd feature\n",
    "xgb_classifier_svd= xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_svd, xtrain_svd, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "xgb_classifier_svd.fit(xtrain_svd, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tfidf svd features\n",
    "xgb_classifier_svd_tfidf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_svd_tfidf, xtrain_svd_scl_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "xgb_classifier_svd_tfidf.fit(xtrain_svd_scl_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[\"target\"]= multinomial_naive_bayes.predict(X_test)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test[\"target\"]= multinomial_naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test.to_csv(\"submission_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
