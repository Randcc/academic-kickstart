{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create textblob objects of the tweets\n",
    "sentiment_objects = [TextBlob(tw) for tw in train['text']]\n",
    "# Create list of polarity valuesx and tweet text\n",
    "sentiment_values = [[tweet.sentiment.polarity] for tweet in sentiment_objects]\n",
    "\n",
    "#Values closer to 1 indicate more positivity, while values closer to -1 indicate more negativity.\n",
    "sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity(_df):\n",
    "    _df = pd.concat([_df, sentiment_df], axis=1)\n",
    "    return _df\n",
    "train=add_polarity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning & Preprocessing tweets Data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_1=[]\n",
    "for tw in train[\"text\"]:\n",
    "    word_tokens = word_tokenize(tw) \n",
    "    #Delete ponctuation\n",
    "    word_tokens=[word.lower() for word in word_tokens if word.isalpha()]\n",
    "    #Delete stop words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words : \n",
    "            if  w!='http':\n",
    "                filtered_sentence.append(w) \n",
    "\n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    lem=[]\n",
    "    for w in filtered_sentence:\n",
    "        word1 = Word(w).lemmatize(\"n\")\n",
    "        word2 = Word(word1).lemmatize(\"v\")\n",
    "        word3 = Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    tweet_1.append(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_objects = [TextBlob(str(t)) for t in tweet_1]\n",
    "sentiment_values = [[tweet_1.sentiment.polarity, str(tweet_1)] for tweet_1 in sentiment_objects]\n",
    "sentiment_values[0]\n",
    "sentiment_df1 = pd.DataFrame(sentiment_values, columns=[\"polarity_lem\", \"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity1(_df):\n",
    "    _df = pd.concat([_df, sentiment_df1[\"lems\"]], axis=1)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=add_polarity1(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lems\"]= train[\"lems\"].str.replace(\"[\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\"]\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\"\\'\", \"\") \n",
    "train[\"lems\"]= train[\"lems\"].str.replace(\",\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning & Preprocessing tweets Data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweet_2=[]\n",
    "for tw in test[\"text\"]:\n",
    "    word_tokens = word_tokenize(tw) \n",
    "    #Delete ponctuation\n",
    "    word_tokens=[word.lower() for word in word_tokens if word.isalpha()]\n",
    "    #Delete stop words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words : \n",
    "            if  w!='http':\n",
    "                filtered_sentence.append(w) \n",
    "    \n",
    "    Stem_words = []\n",
    "    ps =PorterStemmer()\n",
    "    for w in filtered_sentence:\n",
    "        rootWord=ps.stem(w)\n",
    "        Stem_words.append(rootWord)\n",
    "    lem=[]\n",
    "    for w in filtered_sentence:\n",
    "        word1 = Word(w).lemmatize(\"n\")\n",
    "        word2 = Word(word1).lemmatize(\"v\")\n",
    "        word3 = Word(word2).lemmatize(\"a\")\n",
    "        lem.append(Word(word3).lemmatize())\n",
    "    tweet_2.append(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_objects = [TextBlob(str(t)) for t in tweet_2]\n",
    "sentiment_values = [[tweet_2.sentiment.polarity, str(tweet_2)] for tweet_2 in sentiment_objects]\n",
    "sentiment_values[0]\n",
    "sentiment_df1 = pd.DataFrame(sentiment_values, columns=[\"polarity_lem\", \"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity2(_df):\n",
    "    _df = pd.concat([_df, sentiment_df1], axis=1)\n",
    "    return _df\n",
    "test=add_polarity2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"lems\"]= test[\"lems\"].str.replace(\"[\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\"]\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\"\\'\", \"\") \n",
    "test[\"lems\"]= test[\"lems\"].str.replace(\",\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train = count_vectorizer.fit_transform(train[\"lems\"])\n",
    "X_test = count_vectorizer.transform(test[\"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer - Convert text to word frequency vectors.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train[\"lems\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test[\"lems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(clf, X_train, train[\"target\"], cv=3, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression on Count Vectors\n",
    "logistic_reg= LogisticRegression(solver='liblinear')\n",
    "scores = model_selection.cross_val_score(logistic_reg, X_train, train[\"target\"], cv=3, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(X_train,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression on tfidf\n",
    "logistic_reg_tfidf = LogisticRegression(solver='liblinear')\n",
    "scores = model_selection.cross_val_score(logistic_reg_tfidf, X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "\n",
    "logistic_reg_tfidf.fit(X_train_tfidf,train[\"target\"])\n",
    "y_predicted_logistic_reg_tfidf = logistic_reg_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_naive_bayes = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(multinomial_naive_bayes, X_train, train[\"target\"], cv=3, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_naive_bayes.fit( X_train, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes on tfidf\n",
    "multinomial_naive_bayes_tfidf = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(multinomial_naive_bayes_tfidf, X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "multinomial_naive_bayes_tfidf.fit(X_train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM on Count Vector\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "xtrain_svd = svd.fit_transform(X_train)\n",
    "xtest_svd= svd.transform(X_test)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = StandardScaler()\n",
    "xtrain_svd_scl = scl.fit_transform(xtrain_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)\n",
    "# Fitting a simple SVM\n",
    "svm = SVC(gamma='auto')\n",
    "scores = model_selection.cross_val_score(svm , xtrain_svd_scl, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "svm.fit(xtrain_svd_scl,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM on tfidf\n",
    "svd_tfidf = TruncatedSVD(n_components=120)\n",
    "xtrain_svd_tfidf = svd_tfidf.fit_transform(X_train_tfidf)\n",
    "xtest_svd_tfidf = svd_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl_tfidf = StandardScaler()\n",
    "xtrain_svd_scl_tfidf = scl_tfidf.fit_transform(xtrain_svd_tfidf)\n",
    "xtest_svd_scl_tfidf = scl_tfidf.transform(xtest_svd_tfidf)\n",
    "\n",
    "# Fitting a simple SVM\n",
    "svm_tfidf = SVC(gamma='auto')\n",
    "scores = model_selection.cross_val_score(svm_tfidf , xtrain_svd_scl_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "svm_tfidf.fit(xtrain_svd_scl_tfidf,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on Count Vector\n",
    "xgb_classifier = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier, X_train, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "xgb_classifier.fit(X_train,  train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on tfidf\n",
    "xgb_classifier_tfidf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_tfidf , X_train_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "xgb_classifier_tfidf.fit(X_train_tfidf,train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting xgboost on Count Vector svd feature\n",
    "xgb_classifier_svd= xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_svd, xtrain_svd, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "xgb_classifier_svd.fit(xtrain_svd, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=7, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tfidf svd features\n",
    "xgb_classifier_svd_tfidf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(xgb_classifier_svd_tfidf, xtrain_svd_scl_tfidf, train[\"target\"], cv=3, scoring=\"f1\")\n",
    "xgb_classifier_svd_tfidf.fit(xtrain_svd_scl_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"target\"]= multinomial_naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test[\"target\"]= multinomial_naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_test.to_csv(\"submission_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
